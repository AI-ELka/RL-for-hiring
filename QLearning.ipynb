{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Candidate Rejection Step in a Recruitment Process using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The environment :\n",
    "Two actions (Advance or Reject) and a state that consists of the stage of recruitment and the candidate's performance\n",
    "the step method is the effect of the action and gives the reward\n",
    "* The environment represents the recruitment process and has discrete action space (Advance or Reject) and a tuple observation space.\n",
    "* The observation space includes the current stage of recruitment (discrete) and the candidate's performance.\n",
    "* The ``step``  method takes an action, updates the state based on the action\n",
    "* The `reset `method resets the environment to its initial state. \n",
    "* The `render` method displays the current state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class RecruitmentEnv(gym.Env):\n",
    "    def __init__(self,cand):\n",
    "        super(RecruitmentEnv, self).__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)  # Advance or Reject\n",
    "        self.observation_space = gym.spaces.Tuple((\n",
    "            gym.spaces.Discrete(4),  # Stage of recruitment\n",
    "            gym.spaces.Box(low=0, high=1, shape=(4,))  # Candidate performance\n",
    "        ))\n",
    "\n",
    "        self.state=(0,cand)\n",
    "        self.info={}\n",
    "        self.done=False\n",
    "        self.cand=cand\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        good_threshold = 0.7\n",
    "        # bad_threshold = 0.3 # i can add probabilities between the good and bad thresholds\n",
    "        time_penalty = 40\n",
    "\n",
    "        # Get the current candidate's score\n",
    "        rew=0\n",
    "        stage, performance = self.state\n",
    "        \n",
    "        if action == 0:  # Reject the candidate\n",
    "            if performance[stage] > good_threshold: #good candidate\n",
    "                rew= -700 # Penalize for rejecting a good candidate\n",
    "            else:\n",
    "                rew= 600   # Reward for rejecting a bad candidate\n",
    "            \n",
    "            self.done=True\n",
    "            \n",
    "        else:  # Proceed to the next stage\n",
    "            if (stage == 3) & (performance[stage] > good_threshold ):\n",
    "                rew= 500               # Highly reward for successfully extending an offer\n",
    "                \n",
    "                self.done=True\n",
    "            elif (stage == 3) & (performance[stage] < good_threshold ):\n",
    "                rew=300\n",
    "                self.done=True\n",
    "            elif performance[stage] > good_threshold:\n",
    "                rew= 1050                # Reward for moving on with a good candidate\n",
    "                self.state=(stage+1,performance)\n",
    "            else:\n",
    "                rew= -400 - time_penalty * (1+stage)  # Penalize for moving on with a bad candidate\n",
    "                self.state=(stage+1,performance)\n",
    "            rew+=1*(performance[stage]*1000-300)\n",
    "        return self.state, rew, self.done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state=(0,self.cand)\n",
    "        self.done=False\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    # def render(self):\n",
    "    #     # Render the environment to the screen\n",
    "    #     print(f'Step: {self.state[0]}, Scores: {self.state[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The agent:\n",
    "* The Q-learning agent is initialized with hyperparameters such as learning rate, discount factor, exploration rate, and exploration decay rate.\n",
    "* The agent maintains a Q-table to store the expected cumulative rewards for each state-action pair.\n",
    "* The `get_action` method selects an action based on the epsilon-greedy strategy (explore or exploit).\n",
    "* The `train` method updates the Q-value based on the observed reward and the maximum Q-value of the next state.\n",
    "* The exploration rate decays over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.5, discount_factor=0.95, exploration_rate=0.7, exploration_decay_rate=0.99):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "\n",
    "        # Initialize Q-table to 0\n",
    "        self.q_table = np.zeros((5, 2))\n",
    "\n",
    "    def get_action(self, state):   # Epsilon-Greedy Strategy\n",
    "        if np.random.uniform(0, 1) < self.exploration_rate:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def train(self,state,action,reward,next_state):\n",
    "        old_value=self.q_table[state, action]\n",
    "        next_max=np.max(self.q_table[next_state])\n",
    "\n",
    "        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "        # Decay exploration rate\n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate=self.exploration_decay_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop:\n",
    "* A set of candidates with random performance values is generated.\n",
    "* An environment is created for each candidate.\n",
    "* Q-learning agents are initialized for each environment.\n",
    "* The ``training`` function runs episodes of training for each agent on its corresponding environment.\n",
    " - The update rule is given by:\n",
    "     $ Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot \\left(r + \\gamma \\cdot \\max_{a'} Q(s', a')\\right) $\n",
    "     where:\n",
    "     - $Q(s, a)$ is the Q-value for state $s$ and action $a$.\n",
    "     - $\\alpha$ is the learning rate, determines to what extent newly acquired information overrides old information\n",
    "     - $\\gamma$ is the discount factor, determining the importance of future rewards.\n",
    "     - $r$ is the immediate reward received for taking action $a$ in state $s$.\n",
    "     - $s'$ is the next state after taking action $a$.\n",
    "\n",
    "\n",
    "``find_action`` :*The idea is to look for the closest known candidates to our new candidate and do a **majority vote** the find the next action.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tol=100000\n",
    "cands=[np.random.uniform(size=4) for _ in range(tol)]\n",
    "envs=[RecruitmentEnv(cand) for cand in cands]\n",
    "agents=[(i,QLearningAgent(envs[i])) for i in range(tol)]\n",
    "\n",
    "\n",
    "def training(i,agent):\n",
    "\n",
    "\n",
    "    for episode in range(1000):  \n",
    "        state = envs[i].reset()\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            \n",
    "            action = agent.get_action(state[0])\n",
    "            \n",
    "            next_state, reward, done, info = envs[i].step(action)\n",
    "            # envs[i].render() \n",
    "            agent.train(state[0], action, reward, next_state[0])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agents \n",
    "a=[training(i, agent) for i, agent in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96806977 0.14672314 0.35679317 0.58816522]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_action(x,step, cands, agents):\n",
    "    # Calculate the distances\n",
    "    distances =np.array([np.linalg.norm(arr - x) for arr in cands])\n",
    "\n",
    "    # Find the indices of the 5 smallest distances\n",
    "    closest_indices =np.argsort(distances)[:101]\n",
    "\n",
    "    # Get the actions for the closest indices\n",
    "    actions =[agents[i][1].get_action(step) for i in closest_indices]\n",
    "\n",
    "    # Count the occurrences of each action\n",
    "    counts =np.bincount(actions)\n",
    "\n",
    "    # Return the most frequent element\n",
    "    most_frequent_element = np.argmax(counts)\n",
    "\n",
    "    return most_frequent_element\n",
    "\n",
    "x = np.random.uniform(size=4)\n",
    "print(x)\n",
    "find_action(x,0,cands,agents) #  First action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict rejection step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_step(x, cands, agents,log=False):\n",
    "    step=0\n",
    "    done=False\n",
    "    if log:\n",
    "        print(x)\n",
    "    while not done:\n",
    "        action=find_action(x,step,cands,agents)\n",
    "        if log:\n",
    "            print(action , end=\" \")\n",
    "        if action==0:\n",
    "            done=True\n",
    "            break\n",
    "\n",
    "        step+=1\n",
    "    return step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 0.9 0.9 0.9]\n",
      "1 1 1 1 0 Offer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=np.array([0.9,0.9,0.9,0.9])\n",
    "rej=rejection_step(x,cands,agents,log=True)\n",
    "if rej > 3: print('Offer') # 0==profile matching\n",
    "else: print(f'rejection step:{rej}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89677377 0.07774106 0.77534968 0.7285238 ]\n",
      "1 0 rejection step:1\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=4)\n",
    "\n",
    "rej=rejection_step(x,cands,agents,log=True)\n",
    "if rej > 3: print('Offer')\n",
    "else: print(f'rejection step:{rej}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44218574 0.32549752 0.38347218 0.44549719]\n",
      "0 rejection step:0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=4)\n",
    "# x=np.array([0.9,0.9,0.9,0.9])\n",
    "rej=rejection_step(x,cands,agents,log=True)\n",
    "if rej > 3: print('Offer')\n",
    "else: print(f'rejection step:{rej}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
